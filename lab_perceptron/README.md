# Лабораторная работа по курсу "Искусственный интеллект"
# Многослойный персептрон

| Студент | *ФИО* |
|------|------|
| Группа  | *№* |
| Оценка 1 (свой фреймворк) | *X* |
| Оценка 2 (PyTorch/Tensorflow) | *X* |
| Проверил | Сошников Д.В. |

> *Комментарии проверяющего*
### Задание

Решить задачу классификации для датасетов MNIST, FashionMNIST, CIFAR-10 с помощью 1, 2 и 3-слойного персептрона. Попробовать разные передаточные функции, провести сравнительную оценку решений. Решение сделать двумя способами:
* "С нуля", на основе базовых операций библиотеки numpy. Решение желательно реализовать в виде библиотеки, пригодной для решения более широкго круга задач.
* На основе одного из существующих нейросетевых фреймворков, в соответствии с вариантом задания:
   1. PyTorch
   1. Tensorflow/Keras

> *Номер варианта вычисляется по формуле 1 + (N-1) mod 2, где N - номер студента в списке.*

Решение оформить в файлах [Solution_MyFramework.ipynb](Solution_MyFramework.ipynb) и [Solution.ipynb](Solution.ipynb). 
Отчет по работе и сравнение методов пишется в этом файле после задания.
### Критерии оценки

Первая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация сделана как библиотека для обучения сетей различных конфигураций, в соответствии с примером | 1 |
| Улучшена архитектура библиотеки, отдельно вынесены алгоритмы обучения, функции потерь | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |

Вторая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% | 1 |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% | 2 |
| Реализация использует возможности базового фреймворка, включая работу с данными | 3 |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности | 2 |
| Проведен анализ для датасета FashionMNIST | 1 |
| Проведен анализ для другого датасета с цветными картинками (CIFAR-10) | 1 |

## Отчёт по работе

Отчёт, помимо общего описания решения, должен включать:
* График точности на обучающей и проверочной выборке в процессе обучения
* Confusion Matrix
* Сравнение полученных показателей точности модели для различных гиперпараметров (передаточных функций, числа нейронов в слоях и т.д.)

## Codespaces

По возможности, используйте GitHub Codespaces для выполнения работы. По результатам, дайте обратную связь:
1. Что понравилось?
1. Что не понравилось?
1. Какие ошибки или существенные затруднения в работе вы встречали? (По возможности, будьте как можно более подробны, указывайте шаги для воспроизведения ошибок)

## Материалы для изучения

 * [Реализация своего нейросетевого фреймворка](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroMyFw.ipynb)
 * [Введение в PyTorch](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroPyTorch.ipynb)
 * [Введение в Tensorflow/Keras](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroKerasTF.ipynb)

# Отчет по Лабораторной работе №1
# Многослойный персептрон (свой фреймворк)

| Студент | *Федоров Антон Сергеевич* |
|------|------|
| Группа  | *7* |

# Вспомогательные классы и функции
Нейронную сеть буду составлять по кусочкам, описывая составные части. Таким образом можно добиться модульности сети, что существенно упросит конструирование архитектуры и использования разных подходов. В частности, опишу линейный слой, три алгорима его обучения, softmax слой, четыре передаточные функции, три функции потерь. Названия методов указаны в ноутбуке. Все слои нейросети описаны однообразно: имеется конструктор, метод forward, отвечающий за обработку слоем входных результатов, и метод backward, необходимый для обучения и пересчета весов. Линейный слой имеет метод update, который старается уменьшить функцию потерь, согласно заданному алгоритму обучения.

# Класс Net
Подготовив необходимые составляющие, могу описать см класс нейронной сети. В качестве полей он имеет список слоев и функцию потерь. Подразумевается, что функция применяется только в самом конце к предсказанным результатам. Аналогично составляющим сети, имею методы forward и backward, последовательно вызывающие forward и backward у слоев соответственно. Обучение происходит посредством применения метода train_epoch. Он вызывает forward, backward и после подсчета результатов пересчитывает веса на линейных слоях, вызывая update, где это возможно. Для упрощения работы train_epoch спрятан в методе fit, который в цикле вызывает train_epoch. Также добавлены методы для упрощения анализа результатов. get_loss_acc возвращает результат функции потерь и точность модели на переданной выборке. fit_and_plot обучает модель, строя по результатам работы графики. draw_confusion_matrix_and_graphics строит матрицу ошибок.

# Обучение и результаты
Для начала, построю однослойную сеть. Обучение производилось на датасете MNIST. Использовалась однослойная сеть с передаточной функцией ReLU и функцией потерь CrossEntropyLoss. За алгоритм обучения был взят метод градиентного спуска в 10 итераций, learning rate был равен 0.000001.

![Результат однослойной сети](https://sun9-64.userapi.com/impf/115J1Gukrpt8So-yUYnSC7w9px3iYgAhpAlrKQ/1sAzTf0HBaE.jpg?size=1427x603&quality=96&sign=cae5d98e98736b299a66f4638cec6d01&type=album)
![Матрица ошибок однослойной сети](https://sun9-88.userapi.com/impf/xM_OtI4sPIuJMcVu0-nBPepFXmtYmhN5XLHzHg/zFyoK2cMIxw.jpg?size=410x408&quality=96&sign=80fe85f5698cbf6fc28f910ccde49d34&type=album)

Точность предсказаний 85%.

Теперь попробую трехслойную сеть. В промежуточных слоях буду использовать 100 нейронов. Остальные параметры оставлю теми же, только величину число итерацией обучения до 25.

![Результат многослойной сети](https://sun9-45.userapi.com/impf/5T2Oyq8bG5JiImf6XY-7BNcuFDzmg0ZKPo-z_g/EnDdGpbAi_4.jpg?size=1420x605&quality=96&sign=25056f03b307982e30c0c2ccf30f9df6&type=album)
![Матрица ошибок многослойной сети](https://sun1-89.userapi.com/impf/bv4kiR58l3b5oIdjkHQsek9_t22zfrANVB9MXA/nMAnfcNxGXE.jpg?size=410x412&quality=96&sign=f3a7c202820b1de28597908bab60010a&type=album)

Точность предсказаний стала немного лучше.

# Другие гиперпараметры
Менялись различные параметры многослойной сети, взятой в качестве baseline-а. В частности, изменялось число слоев, передаточная функция, функция потерь, число нейронов, алгоритм обучения. Подробные результаты каждого эксперимента приведены в ноутбуке.

Из экспериментов выяснилось, что изначальные параметры сети были довольно удачными. Лучший результат получился на сети с двумя слоями, сотней нейронов, передаточной функцией ReLU, функцией потерь CrossEntropyLoss и алгоритмом обучения NesterovAcceleratedGradient.

![Попытка улучшить результат](https://sun9-36.userapi.com/impf/ArQqtwnpkEO79wjrtx_ha2gkHPRKsM-okFN0Lg/2S9PkaoGLQ8.jpg?size=1405x605&quality=96&sign=9afbed32beb8c621eb1ef28acd4bd9cb&type=album)

# FashionMNIST
Использую baseline-многослойную сеть для анализа датасета FashionMNIST.

![FashionMNIST](https://sun9-52.userapi.com/impf/WNJqcy6ePOtCrVWbu4KC9CBu2OBaSBf2Ta-jVw/UQ09hGlhpmE.jpg?size=1416x605&quality=96&sign=9f37e09ab778a979b307e5e53028ef99&type=album)

Результат 77%. Попробую улучшить. Добавлю слой, увеличу количество нейронов до 150 и число итераций обучения до 50.

![улучшенный FashionMNIST](https://sun9-25.userapi.com/impf/gK8e2gwkFO8hHwei3jF9iYFwR8hmT6yoTjJOjQ/LUlWUGf3C-U.jpg?size=1435x600&quality=96&sign=1acb9875cb5919cc4549972891eb2549&type=album)

Результат стал 81%. Улучшение считаю успешным.


# Многослойный персептрон (PyTorch)

## Подготовка данных для обучения
Загрузка аналогична первой части работы. Были созданы DataLoader-ы для обучающих и тестовых выборок. Размер батча на обучающей выборке - 128, тестовая выдается полностью, так как нужна только для оценки качества нейронной сети.

## Trainer
Для обучения нейронных сетей был реализован специальный класс Trainer. Данный класс имеет метод для обучения train, который принимает сеть, выборки, метод обучения, loss-функцию число итераций и learning rate. Качество модели в процессе обучения сохраняется в полях класса. Эти данные в дальнейшем будут использованы для построения графиков. Также реализована функция отрисовки матрицы ошибок, аналогично первой части. Для ускорения обучения вычисления производятся на GPU.

## Обучение и результаты
Был обучен однослойный персептрон в 15 итераций и learning rate равным 0.001. 
![изображение](https://user-images.githubusercontent.com/62854654/164764859-8e6f694f-87e0-4950-93b1-0b2ffc15c033.png)
![изображение](https://user-images.githubusercontent.com/62854654/164764834-d7fd4d8c-bf1f-4a43-910d-e1cabae1b59a.png)

Затем был обучен многослойный персептрон в 15 итераций. learning rate тот же.
![изображение](https://user-images.githubusercontent.com/62854654/164764956-55a78eda-722a-48a6-a115-d664a1edaf64.png)
![изображение](https://user-images.githubusercontent.com/62854654/164764973-529beb4c-53ee-4974-a4cd-5d8b7300413b.png)

## Другие гиперпараметры
Менялись различные параметры многослойной сети, взятой в качестве baseline-а. В частности, изменялось число слоев, передаточная функция, функция потерь, число нейронов, алгоритм обучения. Подробные результаты каждого эксперимента приведены в ноутбуке.

Лучший результат получился на сети с пятью слоями, двумястами нейронами, передаточной функцией ReLU, функцией потерь CrossEntropyLoss и алгоритмом обучения Adamax. Число итераций - 30, learning rate - 0.0005.
![изображение](https://user-images.githubusercontent.com/62854654/164765374-6fd5555e-31da-41fd-aa7f-207e9bae0301.png)
![изображение](https://user-images.githubusercontent.com/62854654/164765388-60a84874-40f1-4934-b6f1-bf3043203efa.png)

Зезултат получился выше 97%.

## FashionMNIST
Использую лучшую модель для обучения на датасете FashionMNIST.
![изображение](https://user-images.githubusercontent.com/62854654/164766482-68bdd71f-9d0d-4d52-bf18-2ef9f9b88c80.png)
![изображение](https://user-images.githubusercontent.com/62854654/164766494-626d4ca9-d735-4623-910a-69f859241a37.png)

Результат получился выше 85%-в.

## CIFAR-10
Использую лучшую модель для обучения на датасете CIFAR-10.
![изображение](https://user-images.githubusercontent.com/62854654/164767569-7b820481-e78c-4377-b863-d8fb7f033791.png)
![изображение](https://user-images.githubusercontent.com/62854654/164767558-5f80a1eb-8488-49a6-a326-33ad767b689b.png)

Результат оказался сильно хуже, чем на MNIST. Однако, модель хорошо распознает второй и два последних класса. Это оказались самолеты, машины и грузовики, любопытно.

# Вывод
В ходе выполнения лабораторной работы я изучил принципы проектирования и обучения нейронных сетей. Приобрел навыки работы с фреймворком PyTorch. Реализовал классификаторы с использованием PyTorch и с помощью реализации собственного фреймфорка. Провел обучение и тестирование на датасетах MNIST, FashionMNIST и CIFAR-10.

Исходя из проделанной работы, обучение нейросетей кажется довольно простым, если имеется готовый фреймворк. Однако, сложно понять, какие параметры окажутся лучшими, без предварит ной серии экспериментов. По сути, приходится действовать на ощупь. 
